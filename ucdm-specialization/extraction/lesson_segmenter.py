#!/usr/bin/env python3
"""
Segmentador inteligente para las 365 lecciones de UCDM
Extrae cada lecci√≥n individual con su t√≠tulo y contenido completo
"""

import sys
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass

sys.path.append(str(Path(__file__).parent.parent))
from config.settings import *

@dataclass
class UCDMLesson:
    """Estructura de datos para una lecci√≥n de UCDM"""
    number: int
    title: str
    content: str
    position: int
    section: str = "Libro de Ejercicios"
    word_count: int = 0
    char_count: int = 0
    
    def __post_init__(self):
        self.word_count = len(self.content.split())
        self.char_count = len(self.content)

class UCDMLessonSegmenter:
    """Segmentador inteligente para las lecciones de UCDM"""
    
    def __init__(self, content_text: str):
        self.content = content_text
        self.lessons = {}
        self.setup_logging()
        
    def setup_logging(self):
        """Configurar logging"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    def find_lesson_boundaries(self) -> List[Dict]:
        """Encontrar los l√≠mites de cada lecci√≥n en el texto"""
        # Patrones m√°s espec√≠ficos para identificar lecciones
        lesson_patterns = [
            r"(?:\n|^)\s*Lecci√≥n\s+(\d{1,3})\s*[\.:]?\s*(.+?)(?=\n)",
            r"(?:\n|^)\s*LECCI√ìN\s+(\d{1,3})\s*[\.:]?\s*(.+?)(?=\n)",
            r"(?:\n|^)\s*Leccion\s+(\d{1,3})\s*[\.:]?\s*(.+?)(?=\n)"  # Sin tilde
        ]
        
        lesson_boundaries = []
        
        for pattern_idx, pattern in enumerate(lesson_patterns):
            matches = re.finditer(pattern, self.content, re.IGNORECASE | re.MULTILINE)
            
            for match in matches:
                try:
                    lesson_num = int(match.group(1))
                    lesson_title = match.group(2).strip() if match.group(2) else f"Lecci√≥n {lesson_num}"
                    
                    # Filtrar n√∫meros v√°lidos de lecci√≥n
                    if 1 <= lesson_num <= 365:
                        # Validar que no sea solo un n√∫mero de p√°gina
                        context = self.content[max(0, match.start()-50):match.end()+100]
                        
                        # Rechazar si parece ser solo un n√∫mero de p√°gina
                        if len(lesson_title) < 3 and pattern_idx >= 3:
                            # Para patrones de solo n√∫meros, verificar contexto
                            if not re.search(r'(ejercicio|pr√°ctica|repite|afirma)', context, re.IGNORECASE):
                                continue
                        
                        lesson_boundaries.append({
                            'number': lesson_num,
                            'title': lesson_title,
                            'start_pos': match.start(),
                            'end_pos': match.end(),
                            'full_match': match.group(0),
                            'pattern_used': pattern_idx
                        })
                except (ValueError, IndexError):
                    continue
        
        # Eliminar duplicados m√°s inteligentemente
        # Agrupar por n√∫mero de lecci√≥n y elegir el mejor match
        lesson_groups = {}
        for boundary in lesson_boundaries:
            lesson_num = boundary['number']
            if lesson_num not in lesson_groups:
                lesson_groups[lesson_num] = []
            lesson_groups[lesson_num].append(boundary)
        
        unique_boundaries = []
        for lesson_num, candidates in lesson_groups.items():
            # Elegir el mejor candidato para esta lecci√≥n
            # Preferir patrones m√°s espec√≠ficos (√≠ndices menores)
            best_candidate = min(candidates, key=lambda x: (x['pattern_used'], abs(len(x['title']) - 20)))
            unique_boundaries.append(best_candidate)
        
        # Ordenar por posici√≥n en el texto
        unique_boundaries.sort(key=lambda x: x['start_pos'])
        
        self.logger.info(f"Encontrados {len(unique_boundaries)} l√≠mites de lecciones")
        return unique_boundaries
    
    def extract_lesson_content(self, start_boundary: Dict, next_boundary: Optional[Dict] = None) -> str:
        """Extraer el contenido completo de una lecci√≥n"""
        start_pos = start_boundary['end_pos']
        
        # Determinar d√≥nde termina esta lecci√≥n
        if next_boundary:
            # La lecci√≥n termina donde empieza la siguiente
            end_pos = next_boundary['start_pos']
        else:
            # Es la √∫ltima lecci√≥n, buscar el final del Libro de Ejercicios
            # Buscar patrones que indiquen el final con ventana de b√∫squeda
            search_window = 10000  # Buscar en los pr√≥ximos 10k caracteres
            search_text = self.content[start_pos:start_pos + search_window]
            
            end_patterns = [
                r"MANUAL\s+PARA\s+EL\s+MAESTRO",
                r"TERCERA\s+PARTE",
                r"EP√çLOGO",
                r"EPILOGO",
                r"Manual\s+del\s+Maestro",
                r"Tercera\s+Parte"
            ]
            
            end_pos = len(self.content)
            for pattern in end_patterns:
                match = re.search(pattern, search_text, re.IGNORECASE)
                if match:
                    end_pos = start_pos + match.start()
                    break
        
        # Extraer contenido
        raw_content = self.content[start_pos:end_pos].strip()
        
        # Limpiar el contenido
        content = self.clean_lesson_content(raw_content)
        
        return content
    
    def clean_lesson_content(self, raw_content: str) -> str:
        """Limpiar y formatear el contenido de la lecci√≥n"""
        content = raw_content
        
        # Remover n√∫meros de p√°gina aislados
        content = re.sub(r'\n\s*\d+\s*\n', '\n', content)
        content = re.sub(r'^\s*\d+\s*\n', '', content)
        
        # Remover saltos de l√≠nea excesivos
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
        
        # Normalizar espacios pero mantener estructura
        lines = content.split('\n')
        cleaned_lines = []
        for line in lines:
            # Normalizar espacios en cada l√≠nea
            cleaned_line = re.sub(r' +', ' ', line.strip())
            if cleaned_line:  # Solo agregar l√≠neas no vac√≠as
                cleaned_lines.append(cleaned_line)
        
        content = '\n'.join(cleaned_lines)
        
        # Remover contenido que claramente no pertenece a la lecci√≥n
        content = re.sub(r'.*?MANUAL\s+PARA\s+EL\s+MAESTRO.*', '', content, flags=re.IGNORECASE | re.DOTALL)
        content = re.sub(r'.*?TERCERA\s+PARTE.*', '', content, flags=re.IGNORECASE | re.DOTALL)
        
        return content.strip()
    
    def segment_all_lessons(self) -> Dict[int, UCDMLesson]:
        """Segmentar todas las lecciones del Libro de Ejercicios"""
        self.logger.info("Iniciando segmentaci√≥n de lecciones...")
        
        # Encontrar l√≠mites de todas las lecciones
        boundaries = self.find_lesson_boundaries()
        
        if not boundaries:
            self.logger.error("No se encontraron lecciones en el contenido")
            return {}
        
        self.logger.info(f"Encontrados {len(boundaries)} l√≠mites √∫nicos de lecciones")
        
        # Extraer cada lecci√≥n
        lessons = {}
        processed_count = 0
        
        for i, boundary in enumerate(boundaries):
            lesson_num = boundary['number']
            lesson_title = boundary['title']
            
            # Determinar el siguiente l√≠mite
            next_boundary = boundaries[i + 1] if i + 1 < len(boundaries) else None
            
            # Extraer contenido
            content = self.extract_lesson_content(boundary, next_boundary)
            
            # Validar que el contenido tenga sentido
            if len(content.strip()) < 20:  # Muy corto
                self.logger.warning(f"Lecci√≥n {lesson_num} tiene contenido muy corto, omitiendo")
                continue
            
            # Crear objeto de lecci√≥n
            lesson = UCDMLesson(
                number=lesson_num,
                title=lesson_title,
                content=content,
                position=boundary['start_pos']
            )
            
            lessons[lesson_num] = lesson
            processed_count += 1
            
            if processed_count % 50 == 0:
                self.logger.info(f"Procesadas {processed_count} lecciones v√°lidas...")
        
        self.logger.info(f"Segmentaci√≥n completada: {len(lessons)} lecciones extra√≠das")
        return lessons
    
    def validate_lesson_quality(self, lessons: Dict[int, UCDMLesson]) -> Dict[str, any]:
        """Validar la calidad de las lecciones extra√≠das"""
        validation_results = {
            "total_lessons": len(lessons),
            "lesson_numbers": sorted(lessons.keys()),
            "missing_lessons": [],
            "quality_issues": [],
            "statistics": {
                "avg_word_count": 0,
                "min_word_count": float('inf'),
                "max_word_count": 0,
                "total_words": 0
            }
        }
        
        # Identificar lecciones faltantes
        all_lessons = set(range(1, 366))
        found_lessons = set(lessons.keys())
        validation_results["missing_lessons"] = sorted(all_lessons - found_lessons)
        
        # Calcular estad√≠sticas
        if lessons:
            word_counts = [lesson.word_count for lesson in lessons.values()]
            validation_results["statistics"]["avg_word_count"] = sum(word_counts) / len(word_counts)
            validation_results["statistics"]["min_word_count"] = min(word_counts)
            validation_results["statistics"]["max_word_count"] = max(word_counts)
            validation_results["statistics"]["total_words"] = sum(word_counts)
        
        # Identificar lecciones con posibles problemas de calidad
        for lesson_num, lesson in lessons.items():
            # Lecciones muy cortas (menos de 50 palabras)
            if lesson.word_count < 50:
                validation_results["quality_issues"].append({
                    "lesson": lesson_num,
                    "issue": "Contenido muy corto",
                    "word_count": lesson.word_count
                })
            
            # Lecciones sin t√≠tulo claro
            if not lesson.title or len(lesson.title.strip()) < 3:
                validation_results["quality_issues"].append({
                    "lesson": lesson_num,
                    "issue": "T√≠tulo vac√≠o o muy corto",
                    "title": lesson.title
                })
        
        return validation_results
    
    def save_lessons_to_files(self, lessons: Dict[int, UCDMLesson]) -> None:
        """Guardar lecciones en archivos individuales y en √≠ndice"""
        # Crear directorio para lecciones individuales
        lessons_dir = PROCESSED_DATA_DIR / "lessons"
        lessons_dir.mkdir(exist_ok=True)
        
        # Preparar datos para el √≠ndice
        lessons_index = {}
        
        for lesson_num, lesson in lessons.items():
            # Guardar lecci√≥n individual
            lesson_file = lessons_dir / f"lesson_{lesson_num:03d}.txt"
            with open(lesson_file, 'w', encoding='utf-8') as f:
                f.write(f"Lecci√≥n {lesson_num}: {lesson.title}\n")
                f.write("=" * 50 + "\n\n")
                f.write(lesson.content)
            
            # Agregar al √≠ndice
            lessons_index[lesson_num] = {
                "number": lesson.number,
                "title": lesson.title,
                "word_count": lesson.word_count,
                "char_count": lesson.char_count,
                "position": lesson.position,
                "section": lesson.section,
                "file_path": str(lesson_file.relative_to(PROCESSED_DATA_DIR))
            }
        
        # Guardar √≠ndice completo
        index_file = INDICES_DIR / "365_lessons_indexed.json"
        index_file.parent.mkdir(exist_ok=True)
        
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump({
                "metadata": {
                    "total_lessons": len(lessons),
                    "extraction_date": str(datetime.now()),
                    "source": "Un Curso de Milagros - Libro de Ejercicios"
                },
                "lessons": lessons_index
            }, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Lecciones guardadas en: {lessons_dir}")
        self.logger.info(f"√çndice guardado en: {index_file}")

def main():
    """Funci√≥n principal para ejecutar la segmentaci√≥n"""
    # Cargar contenido extra√≠do
    text_file = PROCESSED_DATA_DIR / "ucdm_complete_text.txt"
    
    if not text_file.exists():
        print("‚ùå Error: No se encontr√≥ el archivo de texto extra√≠do")
        print(f"   Ejecuta primero: python extraction/pdf_extractor.py")
        return 1
    
    print("Cargando contenido de UCDM...")
    with open(text_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Crear segmentador
    segmenter = UCDMLessonSegmenter(content)
    
    # Segmentar lecciones
    lessons = segmenter.segment_all_lessons()
    
    if not lessons:
        print("‚ùå Error: No se pudieron extraer lecciones")
        return 1
    
    # Validar calidad
    validation = segmenter.validate_lesson_quality(lessons)
    
    # Guardar resultados
    segmenter.save_lessons_to_files(lessons)
    
    # Guardar reporte de validaci√≥n
    validation_file = PROCESSED_DATA_DIR / "lessons_validation.json"
    with open(validation_file, 'w', encoding='utf-8') as f:
        json.dump(validation, f, indent=2, ensure_ascii=False)
    
    # Mostrar resultados
    print(f"\n{'='*60}")
    print("SEGMENTACI√ìN DE LECCIONES COMPLETADA")
    print(f"{'='*60}")
    
    print(f"\nüìä RESULTADOS:")
    print(f"   Lecciones extra√≠das: {validation['total_lessons']}/365")
    print(f"   Rango de lecciones: {min(validation['lesson_numbers'])}-{max(validation['lesson_numbers'])}")
    
    if validation['missing_lessons']:
        missing_count = len(validation['missing_lessons'])
        print(f"   Lecciones faltantes: {missing_count}")
        if missing_count <= 10:
            print(f"   N√∫meros faltantes: {validation['missing_lessons']}")
        else:
            print(f"   Algunos faltantes: {validation['missing_lessons'][:10]}...")
    
    print(f"\nüìà ESTAD√çSTICAS:")
    stats = validation['statistics']
    print(f"   Promedio de palabras por lecci√≥n: {stats['avg_word_count']:.0f}")
    print(f"   Lecci√≥n m√°s corta: {stats['min_word_count']} palabras")
    print(f"   Lecci√≥n m√°s larga: {stats['max_word_count']} palabras")
    print(f"   Total de palabras: {stats['total_words']:,}")
    
    if validation['quality_issues']:
        print(f"\n‚ö†Ô∏è  PROBLEMAS DE CALIDAD: {len(validation['quality_issues'])}")
        for issue in validation['quality_issues'][:5]:  # Mostrar solo los primeros 5
            print(f"   - Lecci√≥n {issue['lesson']}: {issue['issue']}")
    
    print(f"\n‚úÖ Archivos generados:")
    print(f"   - Lecciones individuales: {PROCESSED_DATA_DIR / 'lessons'}")
    print(f"   - √çndice de lecciones: {INDICES_DIR / '365_lessons_indexed.json'}")
    print(f"   - Reporte de validaci√≥n: {validation_file}")
    
    # Mostrar algunas lecciones de ejemplo
    print(f"\nüìñ EJEMPLOS DE LECCIONES EXTRA√çDAS:")
    example_lessons = [1, 50, 100, 200, 365] if 365 in lessons else [1, 50, 100, 200]
    
    for lesson_num in example_lessons:
        if lesson_num in lessons:
            lesson = lessons[lesson_num]
            print(f"\n   Lecci√≥n {lesson_num}: {lesson.title}")
            print(f"   Palabras: {lesson.word_count}, Caracteres: {lesson.char_count}")
            # Mostrar inicio del contenido
            preview = lesson.content[:100].replace('\n', ' ')
            print(f"   Inicio: {preview}...")
    
    success_rate = (validation['total_lessons'] / 365) * 100
    print(f"\n‚≠ê TASA DE √âXITO: {success_rate:.1f}%")
    
    # Si el √©xito es bajo, intentar an√°lisis adicional
    if success_rate < 50:
        print(f"\nüîç AN√ÅLISIS ADICIONAL (debido a baja tasa de √©xito):")
        
        # Buscar patrones de lecciones m√°s generales
        general_patterns = [
            r'Lecci√≥n\s+\d+',
            r'LECCI√ìN\s+\d+',
            r'\d+\s*[\.-]\s*[A-Z]'
        ]
        
        for i, pattern in enumerate(general_patterns):
            matches = re.findall(pattern, content, re.IGNORECASE)
            print(f"   Patr√≥n {i+1} ('{pattern[:20]}...'): {len(matches)} coincidencias")
        
        # Buscar contenido del Libro de Ejercicios espec√≠ficamente
        workbook_match = re.search(r'LIBRO\s+DE\s+EJERCICIOS', content, re.IGNORECASE)
        if workbook_match:
            workbook_start = workbook_match.end()
            workbook_content = content[workbook_start:workbook_start + 50000]  # Primeros 50k chars
            lesson_count = len(re.findall(r'Lecci√≥n\s+\d+', workbook_content, re.IGNORECASE))
            print(f"   En secci√≥n 'Libro de Ejercicios': {lesson_count} lecciones encontradas")
    
    return 0 if success_rate > 30 else 1  # Reducir umbral para desarrollo

if __name__ == "__main__":
    from datetime import datetime
    import re  # Agregar import faltante
    exit(main())