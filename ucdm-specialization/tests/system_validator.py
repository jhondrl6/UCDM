#!/usr/bin/env python3
"""
Sistema de validaciÃ³n y testing completo para UCDM
Verifica la integridad y funcionalidad de todos los componentes del proyecto
"""

import sys
import json
import unittest
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import logging

sys.path.append(str(Path(__file__).parent.parent))
from config.settings import *
from extraction.pdf_extractor import UCDMPDFExtractor
from training.response_engine import UCDMResponseEngine

class UCDMSystemValidator:
    """Validador completo del sistema UCDM"""
    
    def __init__(self):
        self.results = {}
        self.setup_logging()
        
    def setup_logging(self):
        """Configurar logging para validaciÃ³n"""
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def validate_file_structure(self) -> Dict[str, bool]:
        """Validar estructura de archivos del proyecto"""
        self.logger.info("ğŸ” Validando estructura de archivos...")
        
        required_files = {
            "config/settings.py": PROJECT_ROOT / "config" / "settings.py",
            "extraction/pdf_extractor.py": PROJECT_ROOT / "extraction" / "pdf_extractor.py",
            "extraction/advanced_lesson_segmenter.py": PROJECT_ROOT / "extraction" / "advanced_lesson_segmenter.py",
            "extraction/lesson_indexer.py": PROJECT_ROOT / "extraction" / "lesson_indexer.py",
            "training/dataset_generator.py": PROJECT_ROOT / "training" / "dataset_generator.py",
            "training/response_engine.py": PROJECT_ROOT / "training" / "response_engine.py",
            "ucdm_cli.py": PROJECT_ROOT / "ucdm_cli.py",
            "ollama/Modelfile": PROJECT_ROOT / "ollama" / "Modelfile",
            "ollama/setup_model.py": PROJECT_ROOT / "ollama" / "setup_model.py"
        }
        
        required_dirs = {
            "data": DATA_DIR,
            "data/raw": RAW_DATA_DIR,
            "data/processed": PROCESSED_DATA_DIR,
            "data/training": TRAINING_DATA_DIR,
            "data/indices": INDICES_DIR
        }
        
        validation_results = {}
        
        # Validar archivos
        for name, path in required_files.items():
            exists = path.exists()
            validation_results[f"file_{name}"] = exists
            if exists:
                self.logger.info(f"âœ“ {name}")
            else:
                self.logger.error(f"âœ— {name} - FALTANTE")
        
        # Validar directorios
        for name, path in required_dirs.items():
            exists = path.exists() and path.is_dir()
            validation_results[f"dir_{name}"] = exists
            if exists:
                self.logger.info(f"âœ“ {name}/")
            else:
                self.logger.error(f"âœ— {name}/ - FALTANTE")
        
        return validation_results
    
    def validate_data_extraction(self) -> Dict[str, any]:
        """Validar proceso de extracciÃ³n de datos"""
        self.logger.info("ğŸ“– Validando extracciÃ³n de datos...")
        
        validation_results = {
            "pdf_source_exists": False,
            "extracted_text_exists": False,
            "lessons_indexed": 0,
            "concepts_indexed": 0,
            "integrity_score": 0.0,
            "extraction_log_exists": False
        }
        
        # Verificar PDF fuente
        pdf_path = RAW_DATA_DIR / "Un Curso de Milagros.pdf"
        validation_results["pdf_source_exists"] = pdf_path.exists()
        
        if validation_results["pdf_source_exists"]:
            self.logger.info(f"âœ“ PDF fuente encontrado: {pdf_path}")
        else:
            self.logger.warning(f"âš ï¸ PDF fuente no encontrado: {pdf_path}")
        
        # Verificar texto extraÃ­do
        extracted_text_path = PROCESSED_DATA_DIR / "ucdm_complete_text.txt"
        validation_results["extracted_text_exists"] = extracted_text_path.exists()
        
        if validation_results["extracted_text_exists"]:
            self.logger.info(f"âœ“ Texto extraÃ­do encontrado")
            
            # Verificar tamaÃ±o del archivo
            size_mb = extracted_text_path.stat().st_size / (1024 * 1024)
            self.logger.info(f"   TamaÃ±o: {size_mb:.1f} MB")
        
        # Verificar Ã­ndice de lecciones
        lessons_index_path = INDICES_DIR / "ucdm_comprehensive_index.json"
        if lessons_index_path.exists():
            with open(lessons_index_path, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            validation_results["lessons_indexed"] = len(index_data.get("lesson_details", {}))
            validation_results["concepts_indexed"] = len(index_data.get("concept_index", {}))
            validation_results["integrity_score"] = index_data.get("metadata", {}).get("coverage_percentage", 0) / 100
            
            self.logger.info(f"âœ“ Lecciones indexadas: {validation_results['lessons_indexed']}")
            self.logger.info(f"âœ“ Conceptos indexados: {validation_results['concepts_indexed']}")
            self.logger.info(f"âœ“ Score de integridad: {validation_results['integrity_score']:.2f}")
        
        # Verificar log de extracciÃ³n
        extraction_log_path = RAW_DATA_DIR / "extraction_log.json"
        validation_results["extraction_log_exists"] = extraction_log_path.exists()
        
        return validation_results
    
    def validate_response_engine(self) -> Dict[str, any]:
        """Validar motor de respuestas"""
        self.logger.info("âš¡ Validando motor de respuestas...")
        
        validation_results = {
            "engine_loads": False,
            "data_loaded": False,
            "structure_correct": False,
            "response_quality": 0.0,
            "template_variety": False
        }
        
        try:
            # Crear instancia del motor
            engine = UCDMResponseEngine()
            validation_results["engine_loads"] = True
            self.logger.info("âœ“ Motor de respuestas cargado")
            
            # Cargar datos
            data_loaded = engine.load_data()
            validation_results["data_loaded"] = data_loaded
            
            if data_loaded:
                self.logger.info("âœ“ Datos cargados en el motor")
                
                # Probar generaciÃ³n de respuestas
                test_queries = [
                    "ExplÃ­came la LecciÃ³n 1",
                    "Â¿CuÃ¡l es la lecciÃ³n de hoy?",
                    "HÃ¡blame sobre el perdÃ³n en UCDM"
                ]
                
                structure_checks = []
                response_qualities = []
                
                for query in test_queries:
                    result = engine.process_query(query)
                    response = result.get('response', '')
                    
                    # Verificar estructura
                    required_sections = [
                        "HOOK INICIAL",
                        "APLICACIÃ“N PRÃCTICA",
                        "INTEGRACIÃ“N EXPERIENCIAL", 
                        "CIERRE MOTIVADOR"
                    ]
                    
                    sections_found = sum(1 for section in required_sections if section in response)
                    structure_score = sections_found / len(required_sections)
                    structure_checks.append(structure_score)
                    
                    # Evaluar calidad (longitud, coherencia bÃ¡sica)
                    quality_score = 0.0
                    if len(response) > 200:  # MÃ­nimo de contenido
                        quality_score += 0.3
                    if len(response.split()) > 50:  # MÃ­nimo de palabras
                        quality_score += 0.3
                    if "UCDM" in response or "Curso" in response:  # Contenido relevante
                        quality_score += 0.4
                    
                    response_qualities.append(quality_score)
                
                validation_results["structure_correct"] = sum(structure_checks) / len(structure_checks) > 0.8
                validation_results["response_quality"] = sum(response_qualities) / len(response_qualities)
                
                self.logger.info(f"âœ“ Estructura promedio: {sum(structure_checks)/len(structure_checks):.2f}")
                self.logger.info(f"âœ“ Calidad promedio: {validation_results['response_quality']:.2f}")
                
                # Verificar variedad de templates
                responses = [engine.process_query(q)['response'] for q in test_queries]
                unique_starts = len(set(r[:50] for r in responses))
                validation_results["template_variety"] = unique_starts > 1
                
            else:
                self.logger.error("âœ— No se pudieron cargar datos en el motor")
                
        except Exception as e:
            self.logger.error(f"âœ— Error en motor de respuestas: {str(e)}")
        
        return validation_results
    
    def validate_cli_interface(self) -> Dict[str, bool]:
        """Validar interfaz CLI"""
        self.logger.info("ğŸ’» Validando interfaz CLI...")
        
        validation_results = {
            "cli_file_exists": False,
            "cli_imports_work": False,
            "cli_help_works": False
        }
        
        cli_path = PROJECT_ROOT / "ucdm_cli.py"
        validation_results["cli_file_exists"] = cli_path.exists()
        
        if validation_results["cli_file_exists"]:
            self.logger.info("âœ“ Archivo CLI encontrado")
            
            try:
                # Intentar importar mÃ³dulos necesarios
                import importlib.util
                spec = importlib.util.spec_from_file_location("ucdm_cli", cli_path)
                if spec and spec.loader:
                    validation_results["cli_imports_work"] = True
                    self.logger.info("âœ“ Imports de CLI funcionan")
                
            except Exception as e:
                self.logger.error(f"âœ— Error en imports CLI: {str(e)}")
        
        return validation_results
    
    def validate_dataset_generation(self) -> Dict[str, any]:
        """Validar generaciÃ³n de datasets"""
        self.logger.info("ğŸ“Š Validando generaciÃ³n de datasets...")
        
        validation_results = {
            "dataset_file_exists": False,
            "dataset_size": 0,
            "examples_count": 0,
            "structure_variety": False
        }
        
        # Verificar dataset principal
        dataset_path = TRAINING_DATA_DIR / "ucdm_structured_dataset.jsonl"
        validation_results["dataset_file_exists"] = dataset_path.exists()
        
        if validation_results["dataset_file_exists"]:
            self.logger.info("âœ“ Dataset principal encontrado")
            
            # Analizar contenido del dataset
            examples = []
            try:
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            examples.append(json.loads(line))
                
                validation_results["examples_count"] = len(examples)
                validation_results["dataset_size"] = dataset_path.stat().st_size / (1024 * 1024)  # MB
                
                # Verificar variedad estructural
                if examples:
                    instructions = [ex.get('instruction', '') for ex in examples[:10]]
                    unique_starts = len(set(inst[:20] for inst in instructions))
                    validation_results["structure_variety"] = unique_starts > 3
                
                self.logger.info(f"âœ“ Ejemplos en dataset: {validation_results['examples_count']}")
                self.logger.info(f"âœ“ TamaÃ±o del dataset: {validation_results['dataset_size']:.1f} MB")
                
            except Exception as e:
                self.logger.error(f"âœ— Error analizando dataset: {str(e)}")
        
        return validation_results
    
    def performance_benchmark(self) -> Dict[str, float]:
        """Realizar benchmark de rendimiento"""
        self.logger.info("â±ï¸ Ejecutando benchmark de rendimiento...")
        
        benchmark_results = {
            "engine_load_time": 0.0,
            "response_generation_time": 0.0,
            "cli_startup_time": 0.0
        }
        
        try:
            # Tiempo de carga del motor
            start_time = time.time()
            engine = UCDMResponseEngine()
            engine.load_data()
            benchmark_results["engine_load_time"] = time.time() - start_time
            
            # Tiempo de generaciÃ³n de respuesta
            start_time = time.time()
            result = engine.process_query("ExplÃ­came la LecciÃ³n 1")
            benchmark_results["response_generation_time"] = time.time() - start_time
            
            self.logger.info(f"âœ“ Carga del motor: {benchmark_results['engine_load_time']:.2f}s")
            self.logger.info(f"âœ“ GeneraciÃ³n de respuesta: {benchmark_results['response_generation_time']:.2f}s")
            
        except Exception as e:
            self.logger.error(f"âœ— Error en benchmark: {str(e)}")
        
        return benchmark_results
    
    def run_complete_validation(self) -> Dict[str, any]:
        """Ejecutar validaciÃ³n completa del sistema"""
        self.logger.info("ğŸš€ INICIANDO VALIDACIÃ“N COMPLETA DEL SISTEMA UCDM")
        
        start_time = time.time()
        
        # Ejecutar todas las validaciones
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "file_structure": self.validate_file_structure(),
            "data_extraction": self.validate_data_extraction(),
            "response_engine": self.validate_response_engine(),
            "cli_interface": self.validate_cli_interface(),
            "dataset_generation": self.validate_dataset_generation(),
            "performance": self.performance_benchmark(),
            "total_time": 0.0
        }
        
        self.results["total_time"] = time.time() - start_time
        
        return self.results
    
    def generate_validation_report(self) -> str:
        """Generar reporte de validaciÃ³n"""
        if not self.results:
            return "No hay resultados de validaciÃ³n disponibles"
        
        report = f"""
{'='*80}
ğŸŒŸ REPORTE DE VALIDACIÃ“N DEL SISTEMA UCDM
{'='*80}

ğŸ“… Fecha: {self.results['timestamp']}
â±ï¸  Tiempo total: {self.results['total_time']:.2f} segundos

ğŸ“ ESTRUCTURA DE ARCHIVOS:
"""
        
        # Archivos crÃ­ticos
        critical_files = [
            'file_config/settings.py',
            'file_training/response_engine.py',
            'file_ucdm_cli.py'
        ]
        
        for key, value in self.results['file_structure'].items():
            if key in critical_files:
                status = "âœ…" if value else "âŒ"
                report += f"   {status} {key.replace('file_', '').replace('_', '/')}\n"
        
        # ExtracciÃ³n de datos
        data_ext = self.results['data_extraction']
        report += f"""
ğŸ“– EXTRACCIÃ“N DE DATOS:
   {'âœ…' if data_ext['pdf_source_exists'] else 'âš ï¸'} PDF fuente: {'Encontrado' if data_ext['pdf_source_exists'] else 'No encontrado'}
   {'âœ…' if data_ext['extracted_text_exists'] else 'âŒ'} Texto extraÃ­do: {'Disponible' if data_ext['extracted_text_exists'] else 'No disponible'}
   ğŸ“Š Lecciones indexadas: {data_ext['lessons_indexed']}/365
   ğŸ·ï¸ Conceptos indexados: {data_ext['concepts_indexed']}
   ğŸ“ˆ Score de integridad: {data_ext['integrity_score']:.1%}
"""
        
        # Motor de respuestas
        engine = self.results['response_engine']
        report += f"""
âš¡ MOTOR DE RESPUESTAS:
   {'âœ…' if engine['engine_loads'] else 'âŒ'} Carga del motor: {'OK' if engine['engine_loads'] else 'Error'}
   {'âœ…' if engine['data_loaded'] else 'âŒ'} Datos cargados: {'OK' if engine['data_loaded'] else 'Error'}
   {'âœ…' if engine['structure_correct'] else 'âŒ'} Estructura correcta: {'OK' if engine['structure_correct'] else 'Error'}
   ğŸ“Š Calidad de respuestas: {engine['response_quality']:.1%}
   ğŸ¨ Variedad de templates: {'OK' if engine['template_variety'] else 'Limitada'}
"""
        
        # Dataset
        dataset = self.results['dataset_generation']
        report += f"""
ğŸ“Š GENERACIÃ“N DE DATASETS:
   {'âœ…' if dataset['dataset_file_exists'] else 'âŒ'} Dataset principal: {'Disponible' if dataset['dataset_file_exists'] else 'No encontrado'}
   ğŸ“ˆ Ejemplos generados: {dataset['examples_count']}
   ğŸ’¾ TamaÃ±o del dataset: {dataset['dataset_size']:.1f} MB
   ğŸ¯ Variedad estructural: {'OK' if dataset['structure_variety'] else 'Limitada'}
"""
        
        # Rendimiento
        perf = self.results['performance']
        report += f"""
â±ï¸ RENDIMIENTO:
   ğŸš€ Carga del motor: {perf['engine_load_time']:.2f}s
   ğŸ’­ GeneraciÃ³n de respuesta: {perf['response_generation_time']:.2f}s
"""
        
        # Resumen general
        total_checks = 0
        passed_checks = 0
        
        for category in ['file_structure', 'data_extraction', 'response_engine', 'cli_interface', 'dataset_generation']:
            for key, value in self.results[category].items():
                if isinstance(value, bool):
                    total_checks += 1
                    if value:
                        passed_checks += 1
                elif isinstance(value, (int, float)) and key in ['lessons_indexed', 'concepts_indexed']:
                    total_checks += 1
                    if value > 0:
                        passed_checks += 1
        
        success_rate = (passed_checks / total_checks) * 100 if total_checks > 0 else 0
        
        report += f"""
{'='*80}
ğŸ“‹ RESUMEN GENERAL:
   âœ… Checks pasados: {passed_checks}/{total_checks}
   ğŸ“Š Tasa de Ã©xito: {success_rate:.1f}%
   
ğŸ¯ ESTADO DEL SISTEMA: {'ğŸŸ¢ OPERATIVO' if success_rate > 80 else 'ğŸŸ¡ PARCIAL' if success_rate > 60 else 'ğŸ”´ REQUIERE ATENCIÃ“N'}

ğŸ’¡ RECOMENDACIONES:
"""
        
        if data_ext['lessons_indexed'] < 300:
            report += "   â€¢ Ejecutar extracciÃ³n completa del PDF\n"
        
        if not engine['structure_correct']:
            report += "   â€¢ Verificar templates del motor de respuestas\n"
        
        if dataset['examples_count'] < 500:
            report += "   â€¢ Generar mÃ¡s ejemplos de entrenamiento\n"
        
        if perf['response_generation_time'] > 5:
            report += "   â€¢ Optimizar rendimiento del motor\n"
        
        report += f"""
ğŸš€ PASOS SIGUIENTES:
   1. Revisar elementos marcados como âŒ
   2. Ejecutar scripts de configuraciÃ³n faltantes
   3. Probar integraciÃ³n con Ollama
   4. Validar respuestas del modelo especializado

{'='*80}
"""
        
        return report
    
    def save_validation_results(self) -> str:
        """Guardar resultados de validaciÃ³n"""
        results_file = PROJECT_ROOT / "validation_report.json"
        report_file = PROJECT_ROOT / "validation_report.txt"
        
        # Guardar JSON
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # Guardar reporte de texto
        report_text = self.generate_validation_report()
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        self.logger.info(f"âœ… Resultados guardados en: {results_file}")
        self.logger.info(f"âœ… Reporte guardado en: {report_file}")
        
        return str(report_file)

def main():
    """FunciÃ³n principal de validaciÃ³n"""
    print("ğŸ” Iniciando validaciÃ³n del sistema UCDM...")
    
    validator = UCDMSystemValidator()
    
    # Ejecutar validaciÃ³n completa
    results = validator.run_complete_validation()
    
    # Generar y mostrar reporte
    report = validator.generate_validation_report()
    print(report)
    
    # Guardar resultados
    report_file = validator.save_validation_results()
    
    # Determinar cÃ³digo de salida
    success_checks = 0
    total_checks = 0
    
    for category in results:
        if isinstance(results[category], dict):
            for key, value in results[category].items():
                if isinstance(value, bool):
                    total_checks += 1
                    if value:
                        success_checks += 1
    
    success_rate = (success_checks / total_checks) * 100 if total_checks > 0 else 0
    
    if success_rate > 80:
        print("ğŸ‰ SISTEMA VALIDADO EXITOSAMENTE")
        return 0
    elif success_rate > 60:
        print("âš ï¸ SISTEMA PARCIALMENTE FUNCIONAL")
        return 1
    else:
        print("âŒ SISTEMA REQUIERE ATENCIÃ“N")
        return 2

if __name__ == "__main__":
    exit(main())